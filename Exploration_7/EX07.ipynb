{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "incorporated-antique",
   "metadata": {},
   "source": [
    "## 1.데이터 준비와 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "strange-rescue",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import urllib.request\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from collections import Counter\n",
    "\n",
    "# 데이터를 읽어온다. \n",
    "train_data = pd.read_table('~/aiffel/sentiment_classification/data/ratings_train.txt')\n",
    "test_data = pd.read_table('~/aiffel/sentiment_classification/data/ratings_test.txt')\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simple-dealing",
   "metadata": {},
   "source": [
    "## 2.데이터로더 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medieval-communist",
   "metadata": {},
   "source": [
    "실습 때 다루었던 IMDB 데이터셋은 텍스트를 가공하여 imdb.data_loader() 메소드를 호출하면 숫자 인덱스로 변환된 텍스트와 word_to_index 딕셔너리까지 친절하게 제공합니다. 그러나 이번에 다루게 될 nsmc 데이터셋은 전혀 가공되지 않은 텍스트 파일로 이루어져 있습니다. 이것을 읽어서 imdb.data_loader()와 동일하게 동작하는 자신만의 data_loader를 만들어 보는 것으로 시작합니다. data_loader 안에서는 다음을 수행해야 합니다.\n",
    "\n",
    "데이터의 중복 제거\n",
    "NaN 결측치 제거\n",
    "한국어 토크나이저로 토큰화\n",
    "불용어(Stopwords) 제거\n",
    "사전word_to_index 구성\n",
    "텍스트 스트링을 사전 인덱스 스트링으로 변환\n",
    "X_train, y_train, X_test, y_test, word_to_index 리턴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adverse-namibia",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "tokenizer = Mecab()\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "def load_data(train_data, test_data, num_words=10000):\n",
    "    train_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "    train_data = train_data.dropna(how = 'any') \n",
    "    test_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "    test_data = test_data.dropna(how = 'any') \n",
    "    \n",
    "    X_train = []\n",
    "    for sentence in train_data['document']:\n",
    "        temp_X = tokenizer.morphs(sentence) # 토큰화\n",
    "        temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "        X_train.append(temp_X)\n",
    "\n",
    "    X_test = []\n",
    "    for sentence in test_data['document']:\n",
    "        temp_X = tokenizer.morphs(sentence) # 토큰화\n",
    "        temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "        X_test.append(temp_X)\n",
    "    \n",
    "    words = np.concatenate(X_train).tolist()\n",
    "    counter = Counter(words)\n",
    "    counter = counter.most_common(10000-4)\n",
    "    vocab = ['', '', '', ''] + [key for key, _ in counter]\n",
    "    word_to_index = {word:index for index, word in enumerate(vocab)}\n",
    "        \n",
    "    def wordlist_to_indexlist(wordlist):\n",
    "        return [word_to_index[word] if word in word_to_index else word_to_index[''] for word in wordlist]\n",
    "        \n",
    "    X_train = list(map(wordlist_to_indexlist, X_train))\n",
    "    X_test = list(map(wordlist_to_indexlist, X_test))\n",
    "        \n",
    "    return X_train, np.array(list(train_data['label'])), X_test, np.array(list(test_data['label'])), word_to_index\n",
    "    \n",
    "X_train, y_train, X_test, y_test, word_to_index = load_data(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caring-general",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#실제 인코딩 인덱스는 제공된 word_to_index에서 index 기준으로 3씩 뒤로 밀려 있습니다.  \n",
    "word_to_index = {k:(v+3) for k,v in word_to_index.items()}\n",
    "\n",
    "# 처음 몇 개 인덱스는 사전에 정의되어 있습니다\n",
    "word_to_index[\"<PAD>\"] = 0\n",
    "word_to_index[\"<BOS>\"] = 1\n",
    "word_to_index[\"<UNK>\"] = 2  # unknown\n",
    "word_to_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "index_to_word = {index:word for word, index in word_to_index.items()}\n",
    "\n",
    "index_to_word[0] = \"<PAD>\"\n",
    "index_to_word[1] = \"<BOS>\"\n",
    "index_to_word[2] = \"<UNK>\"\n",
    "index_to_word[3] = \"<UNUSED>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afraid-edmonton",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 문장 1개를 활용할 딕셔너리와 함께 주면, 단어 인덱스 리스트 벡터로 변환해 주는 함수\n",
    "# 단, 모든 문장은 <BOS>로 시작한다. \n",
    "def get_encoded_sentence(sentence, word_to_index):\n",
    "    return [word_to_index['<BOS>']]+[word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in sentence.split()]\n",
    "\n",
    "# 여러 개의 문장 리스트를 한꺼번에 단어 인덱스 리스트 벡터로 encode해 주는 함수 \n",
    "def get_encoded_sentences(sentences, word_to_index):\n",
    "    return [get_encoded_sentence(sentence, word_to_index) for sentence in sentences]\n",
    "\n",
    "# 숫자 벡터로 encode된 문장을 원래대로 decode하는 함수\n",
    "def get_decoded_sentence(encoded_sentence, index_to_word):\n",
    "    return ' '.join(index_to_word[index] if index in index_to_word else '<UNK>' for index in encoded_sentence[1:])  #[1:]를 통해 <BOS>를 제외\n",
    "\n",
    "# 여러 개의 숫자 벡터로 encode된 문장을 한꺼번에 원래대로 decode하는 함수\n",
    "def get_decoded_sentences(encoded_sentences, index_to_word):\n",
    "    return [get_decoded_sentence(encoded_sentence, index_to_word) for encoded_sentence in encoded_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuing-shadow",
   "metadata": {},
   "source": [
    "## 3. 모델구성을 위한 데이터 분석 및 가공"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improving-shannon",
   "metadata": {},
   "source": [
    "데이터셋 내 문장 길이 분포\n",
    "적절한 최대 문장 길이 지정\n",
    "keras.preprocessing.sequence.pad_sequences 을 활용한 패딩 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "considered-shanghai",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장길이 평균 :  15.96940191154864\n",
      "문장길이 최대 :  116\n",
      "문장길이 표준편차 :  12.843571191092\n",
      "pad_sequences maxlen :  41\n",
      "전체 문장의 0.9342988343341575%가 maxlen 설정값 이내에 포함됩니다. \n"
     ]
    }
   ],
   "source": [
    "\n",
    "total_data_text = list(X_train) + list(X_test)\n",
    "# 텍스트데이터 문장길이의 리스트를 생성한 후\n",
    "num_tokens = [len(tokens) for tokens in total_data_text]\n",
    "num_tokens = np.array(num_tokens)\n",
    "# 문장길이의 평균값, 최대값, 표준편차를 계산해 본다. \n",
    "print('문장길이 평균 : ', np.mean(num_tokens))\n",
    "print('문장길이 최대 : ', np.max(num_tokens))\n",
    "print('문장길이 표준편차 : ', np.std(num_tokens))\n",
    "\n",
    "# 예를들어, 최대 길이를 (평균 + 2*표준편차)로 한다면,  \n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "maxlen = int(max_tokens)\n",
    "print('pad_sequences maxlen : ', maxlen)\n",
    "print('전체 문장의 {}%가 maxlen 설정값 이내에 포함됩니다. '.format(np.sum(num_tokens < max_tokens) / len(num_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "advance-helicopter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(146182, 41)\n"
     ]
    }
   ],
   "source": [
    "X_train = keras.preprocessing.sequence.pad_sequences(X_train,\n",
    "                                                        value=word_to_index[\"<PAD>\"],\n",
    "                                                        padding='pre', # 혹은 'pre'\n",
    "                                                        maxlen=maxlen)\n",
    "\n",
    "X_test = keras.preprocessing.sequence.pad_sequences(X_test,\n",
    "                                                       value=word_to_index[\"<PAD>\"],\n",
    "                                                       padding='pre', # 혹은 'pre'\n",
    "                                                       maxlen=maxlen)\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "substantial-richardson",
   "metadata": {},
   "source": [
    "## 4. 모델구성 및 validation set 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lightweight-optics",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "tough-capture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 16)          160000    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 8)                 800       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 160,881\n",
      "Trainable params: 160,881\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 16  # 워드 벡터의 차원 수 (변경 가능한 하이퍼파라미터)\n",
    "\n",
    "# model 설계 - 딥러닝 모델 코드를 직접 작성해 주세요.\n",
    "model = keras.Sequential()\n",
    "# [[YOUR CODE]]\n",
    "model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(keras.layers.LSTM(8))   # 가장 널리 쓰이는 RNN인 LSTM 레이어를 사용하였습니다. 이때 LSTM state 벡터의 차원수는 8로 하였습니다. (변경 가능)\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "mathematical-spray",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(136182, 41)\n",
      "(136182,)\n"
     ]
    }
   ],
   "source": [
    "# validation set 10000건 분리\n",
    "x_val = X_train[:10000]   \n",
    "y_val = y_train[:10000]\n",
    "\n",
    "# validation set을 제외한 나머지 15000건\n",
    "partial_X_train = X_train[10000:]  \n",
    "partial_y_train = y_train[10000:]\n",
    "\n",
    "print(partial_X_train.shape)\n",
    "print(partial_y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "former-berlin",
   "metadata": {},
   "source": [
    "### CNN - Conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "tropical-dublin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 16)          160000    \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, None, 16)          1808      \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, None, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, None, 16)          1808      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 163,761\n",
      "Trainable params: 163,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10000  # 어휘 사전의 크기입니다(10개의 단어)\n",
    "word_vector_dim = 16   # 단어 하나를 표현하는 임베딩 벡터의 차원 수입니다. \n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(keras.layers.MaxPooling1D(5))\n",
    "model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(keras.layers.GlobalMaxPooling1D())\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "integrated-belgium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(136182, 41)\n",
      "(136182,)\n"
     ]
    }
   ],
   "source": [
    "# validation set 10000건 분리\n",
    "x_val = X_train[:10000]   \n",
    "y_val = y_train[:10000]\n",
    "\n",
    "# validation set을 제외한 나머지 15000건\n",
    "partial_X_train = X_train[10000:]  \n",
    "partial_y_train = y_train[10000:]\n",
    "\n",
    "print(partial_X_train.shape)\n",
    "print(partial_y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compliant-collect",
   "metadata": {},
   "source": [
    "### CNN - GlobalMaxPooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "equivalent-baghdad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 16)          160000    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 160,145\n",
      "Trainable params: 160,145\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10000 # 어휘 사전의 크기입니다(10개의 단어)\n",
    "word_vector_dim = 16   # 단어 하나를 표현하는 임베딩 벡터의 차원 수입니다. \n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(keras.layers.GlobalMaxPooling1D())\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "adopted-raleigh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(136182, 41)\n",
      "(136182,)\n"
     ]
    }
   ],
   "source": [
    "# validation set 10000건 분리\n",
    "x_val = X_train[:10000]   \n",
    "y_val = y_train[:10000]\n",
    "\n",
    "# validation set을 제외한 나머지 15000건\n",
    "partial_X_train = X_train[10000:]  \n",
    "partial_y_train = y_train[10000:]\n",
    "\n",
    "print(partial_X_train.shape)\n",
    "print(partial_y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binary-faith",
   "metadata": {},
   "source": [
    "## 5.모델 훈련 개시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "industrial-characteristic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "266/266 [==============================] - 3s 7ms/step - loss: 0.6632 - accuracy: 0.6037 - val_loss: 0.4937 - val_accuracy: 0.8152\n",
      "Epoch 2/20\n",
      "266/266 [==============================] - 1s 5ms/step - loss: 0.4409 - accuracy: 0.8335 - val_loss: 0.3618 - val_accuracy: 0.8431\n",
      "Epoch 3/20\n",
      "266/266 [==============================] - 1s 5ms/step - loss: 0.3329 - accuracy: 0.8626 - val_loss: 0.3452 - val_accuracy: 0.8452\n",
      "Epoch 4/20\n",
      "266/266 [==============================] - 1s 6ms/step - loss: 0.3011 - accuracy: 0.8761 - val_loss: 0.3438 - val_accuracy: 0.8478\n",
      "Epoch 5/20\n",
      "266/266 [==============================] - 1s 6ms/step - loss: 0.2797 - accuracy: 0.8863 - val_loss: 0.3468 - val_accuracy: 0.8475\n",
      "Epoch 6/20\n",
      "266/266 [==============================] - 2s 6ms/step - loss: 0.2630 - accuracy: 0.8945 - val_loss: 0.3529 - val_accuracy: 0.8463\n",
      "Epoch 7/20\n",
      "266/266 [==============================] - 2s 6ms/step - loss: 0.2457 - accuracy: 0.9023 - val_loss: 0.3603 - val_accuracy: 0.8457\n",
      "Epoch 8/20\n",
      "266/266 [==============================] - 1s 6ms/step - loss: 0.2359 - accuracy: 0.9078 - val_loss: 0.3693 - val_accuracy: 0.8431\n",
      "Epoch 9/20\n",
      "266/266 [==============================] - 1s 6ms/step - loss: 0.2240 - accuracy: 0.9123 - val_loss: 0.3778 - val_accuracy: 0.8447\n",
      "Epoch 10/20\n",
      "266/266 [==============================] - 1s 6ms/step - loss: 0.2127 - accuracy: 0.9174 - val_loss: 0.3874 - val_accuracy: 0.8418\n",
      "Epoch 11/20\n",
      "266/266 [==============================] - 2s 6ms/step - loss: 0.2088 - accuracy: 0.9210 - val_loss: 0.3988 - val_accuracy: 0.8428\n",
      "Epoch 12/20\n",
      "266/266 [==============================] - 1s 5ms/step - loss: 0.2003 - accuracy: 0.9251 - val_loss: 0.4098 - val_accuracy: 0.8402\n",
      "Epoch 13/20\n",
      "266/266 [==============================] - 2s 6ms/step - loss: 0.1882 - accuracy: 0.9305 - val_loss: 0.4199 - val_accuracy: 0.8376\n",
      "Epoch 14/20\n",
      "266/266 [==============================] - 1s 5ms/step - loss: 0.1833 - accuracy: 0.9324 - val_loss: 0.4307 - val_accuracy: 0.8355\n",
      "Epoch 15/20\n",
      "266/266 [==============================] - 1s 5ms/step - loss: 0.1769 - accuracy: 0.9345 - val_loss: 0.4414 - val_accuracy: 0.8350\n",
      "Epoch 16/20\n",
      "266/266 [==============================] - 1s 5ms/step - loss: 0.1729 - accuracy: 0.9366 - val_loss: 0.4519 - val_accuracy: 0.8352\n",
      "Epoch 17/20\n",
      "266/266 [==============================] - 1s 5ms/step - loss: 0.1688 - accuracy: 0.9390 - val_loss: 0.4629 - val_accuracy: 0.8325\n",
      "Epoch 18/20\n",
      "266/266 [==============================] - 1s 5ms/step - loss: 0.1628 - accuracy: 0.9410 - val_loss: 0.4764 - val_accuracy: 0.8334\n",
      "Epoch 19/20\n",
      "266/266 [==============================] - 1s 6ms/step - loss: 0.1596 - accuracy: 0.9435 - val_loss: 0.4866 - val_accuracy: 0.8320\n",
      "Epoch 20/20\n",
      "266/266 [==============================] - 2s 6ms/step - loss: 0.1536 - accuracy: 0.9457 - val_loss: 0.4939 - val_accuracy: 0.8319\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=20  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "history = model.fit(partial_X_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "pharmaceutical-garage",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "unlike-milton",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1537/1537 - 2s - loss: 0.5166 - accuracy: 0.8271\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-b1f849a2332b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mesults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "esults = model.evaluate(X_test,  y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "western-hurricane",
   "metadata": {},
   "source": [
    "## 6. 학습된 Embedding 레이어 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "superior-stations",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 16)\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = model.layers[0]\n",
    "weights = embedding_layer.get_weights()[0]\n",
    "print(weights.shape)    # shape: (vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "furnished-fantasy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 학습한 Embedding 파라미터를 파일에 써서 저장한다. \n",
    "word2vec_file_path = os.getenv('HOME')+'/aiffel/sentiment_classification/data/word2vec.txt'\n",
    "f = open(word2vec_file_path, 'w')\n",
    "f.write('{} {}\\n'.format(vocab_size-6, word_vector_dim))  # 몇개의 벡터를 얼마 사이즈로 기재할지 타이틀을 쓴다.\n",
    "\n",
    "# 단어 개수(에서 특수문자 4개는 제외하고)만큼의 워드 벡터를 파일에 기록한다.\n",
    "vectors = model.get_weights()[0]\n",
    "for i in range(6, vocab_size):\n",
    "    f.write('{} {}\\n'.format(index_to_word[i], ' '.join(map(str, list(vectors[i, :])))))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "greatest-thanks",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.08743397, -0.20292056, -0.25287682,  0.18673502, -0.04719535,\n",
       "       -0.23462707, -0.3622341 ,  0.12811421, -0.07080534, -0.28198147,\n",
       "       -0.36038268, -0.09573516, -0.18638298, -0.25343537, -0.2757067 ,\n",
       "       -0.03392104], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
    "\n",
    "word2vec = Word2VecKeyedVectors.load_word2vec_format(word2vec_file_path, binary=False)\n",
    "vector = word2vec['사람']\n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "animal-information",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('십', 0.8976855874061584),\n",
       " ('차', 0.8941438794136047),\n",
       " ('통해', 0.8773378729820251),\n",
       " ('간다', 0.8686038255691528),\n",
       " ('뭘', 0.8669915795326233),\n",
       " ('뗄', 0.8533217310905457),\n",
       " ('완벽', 0.8447156548500061),\n",
       " ('훨', 0.8365986347198486),\n",
       " ('...?', 0.8302752375602722),\n",
       " ('어느', 0.82895827293396)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.similar_by_word(\"사람\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "southwest-habitat",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 16  # 워드 벡터의 차원수 (변경가능한 하이퍼파라미터)\n",
    "\n",
    "embedding_matrix = np.random.rand(vocab_size, word_vector_dim)\n",
    "\n",
    "# embedding_matrix에 Word2Vec 워드 벡터를 단어 하나씩마다 차례차례 카피한다.\n",
    "for i in range(6, vocab_size):\n",
    "    if index_to_word[i] in word2vec:\n",
    "        embedding_matrix[i] = word2vec[index_to_word[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "higher-prague",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 41, 16)            160000    \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 35, 16)            1808      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 7, 16)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 1, 16)             1808      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 163,761\n",
      "Trainable params: 163,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.initializers import Constant\n",
    "\n",
    "vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 16  # 워드 벡터의 차원 수 (변경가능한 하이퍼파라미터)\n",
    "\n",
    "# 모델 구성\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, \n",
    "                                 word_vector_dim, \n",
    "                                 embeddings_initializer=Constant(embedding_matrix),  # 카피한 임베딩을 여기서 활용\n",
    "                                 input_length=maxlen, \n",
    "                                 trainable=True))   # trainable을 True로 주면 Fine-tuning\n",
    "model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(keras.layers.MaxPooling1D(5))\n",
    "model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(keras.layers.GlobalMaxPooling1D())\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid')) \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "seeing-swing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "266/266 [==============================] - 8s 17ms/step - loss: 0.6032 - accuracy: 0.6513 - val_loss: 0.3562 - val_accuracy: 0.8418\n",
      "Epoch 2/20\n",
      "266/266 [==============================] - 2s 7ms/step - loss: 0.3365 - accuracy: 0.8568 - val_loss: 0.3437 - val_accuracy: 0.8497\n",
      "Epoch 3/20\n",
      "266/266 [==============================] - 2s 8ms/step - loss: 0.3091 - accuracy: 0.8708 - val_loss: 0.3426 - val_accuracy: 0.8512\n",
      "Epoch 4/20\n",
      "266/266 [==============================] - 2s 8ms/step - loss: 0.2941 - accuracy: 0.8772 - val_loss: 0.3477 - val_accuracy: 0.8490\n",
      "Epoch 5/20\n",
      "266/266 [==============================] - 2s 8ms/step - loss: 0.2779 - accuracy: 0.8876 - val_loss: 0.3492 - val_accuracy: 0.8481\n",
      "Epoch 6/20\n",
      "266/266 [==============================] - 2s 8ms/step - loss: 0.2678 - accuracy: 0.8911 - val_loss: 0.3516 - val_accuracy: 0.8499\n",
      "Epoch 7/20\n",
      "266/266 [==============================] - 2s 8ms/step - loss: 0.2536 - accuracy: 0.8988 - val_loss: 0.3619 - val_accuracy: 0.8468\n",
      "Epoch 8/20\n",
      "266/266 [==============================] - 2s 8ms/step - loss: 0.2378 - accuracy: 0.9082 - val_loss: 0.3635 - val_accuracy: 0.8466\n",
      "Epoch 9/20\n",
      "266/266 [==============================] - 2s 8ms/step - loss: 0.2242 - accuracy: 0.9140 - val_loss: 0.3803 - val_accuracy: 0.8472\n",
      "Epoch 10/20\n",
      "266/266 [==============================] - 2s 8ms/step - loss: 0.2083 - accuracy: 0.9210 - val_loss: 0.3891 - val_accuracy: 0.8452\n",
      "Epoch 11/20\n",
      "266/266 [==============================] - 2s 8ms/step - loss: 0.1904 - accuracy: 0.9308 - val_loss: 0.4036 - val_accuracy: 0.8419\n",
      "Epoch 12/20\n",
      "266/266 [==============================] - 2s 8ms/step - loss: 0.1747 - accuracy: 0.9379 - val_loss: 0.4181 - val_accuracy: 0.8432\n",
      "Epoch 13/20\n",
      "266/266 [==============================] - 2s 8ms/step - loss: 0.1608 - accuracy: 0.9456 - val_loss: 0.4442 - val_accuracy: 0.8390\n",
      "Epoch 14/20\n",
      "266/266 [==============================] - 2s 8ms/step - loss: 0.1463 - accuracy: 0.9518 - val_loss: 0.4658 - val_accuracy: 0.8385\n",
      "Epoch 15/20\n",
      "266/266 [==============================] - 2s 8ms/step - loss: 0.1306 - accuracy: 0.9586 - val_loss: 0.5125 - val_accuracy: 0.8371\n",
      "Epoch 16/20\n",
      "266/266 [==============================] - 2s 8ms/step - loss: 0.1198 - accuracy: 0.9634 - val_loss: 0.5198 - val_accuracy: 0.8334\n",
      "Epoch 17/20\n",
      "266/266 [==============================] - 2s 8ms/step - loss: 0.1125 - accuracy: 0.9667 - val_loss: 0.5535 - val_accuracy: 0.8311\n",
      "Epoch 18/20\n",
      "266/266 [==============================] - 2s 8ms/step - loss: 0.1003 - accuracy: 0.9711 - val_loss: 0.5604 - val_accuracy: 0.8294\n",
      "Epoch 19/20\n",
      "266/266 [==============================] - 2s 8ms/step - loss: 0.0937 - accuracy: 0.9731 - val_loss: 0.5940 - val_accuracy: 0.8291\n",
      "Epoch 20/20\n",
      "266/266 [==============================] - 2s 8ms/step - loss: 0.0871 - accuracy: 0.9754 - val_loss: 0.6232 - val_accuracy: 0.8302\n"
     ]
    }
   ],
   "source": [
    "# 학습의 진행\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=20  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "history = model.fit(partial_X_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cognitive-sterling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1537/1537 - 4s - loss: 0.6484 - accuracy: 0.8244\n",
      "[0.6483868956565857, 0.824379026889801]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test,  y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "productive-haven",
   "metadata": {},
   "source": [
    "## 결론\n",
    "\n",
    "train 데이터의 크기가 작아서 loss 값이 작게 나왔다. 그렇지만, test 데이터를 평가하면, loss 값이 train 데이터 때보다 크다.\n",
    "RNN모델을 사용했을 때, loss와 accuracy 값이 안정적이기 때문에 데이터가 가장 적합한 모델은 RNN이라고 생각한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excessive-color",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
